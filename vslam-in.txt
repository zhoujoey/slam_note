一 刚体运动
	1. 位姿不同表示间的相互转化、旋转矩阵特征值和特征向量物理意义，自由度？
		旋转矩阵 --》 行列式为1的正交矩阵 , 特征值是运动速度，特征向量是运动方向 九个量描述三个自由度
		变换矩阵 --》 16个量表达6自由度 本身带约束，求解困难
		旋转向量 --》 罗德里格斯公式 方向=旋转方向，大小=旋转量， 三维向量表述三自由度
		欧拉角 --》万向锁 旋转分解为xyz上的转角，奇异性问题：当pitch=90，第一次旋转与第三次旋转使用同一轴，丢失一个自由度。
		四元数 --》旋转矩阵冗余，旋转向量+欧拉角 奇异性。四元数1实3虚 互相转换。
		
	2. 欧氏变换，相似变换、仿射变换、射影变换的区别
		欧氏变换 [R,t,0,1]  R为行列式为1 正交矩阵。平移+旋转
		相似变换 [sR,t,0,1] 平移+旋转+缩放
		仿射变换[A,t,0,1] A对称阵，平行性质
		射影变换[A,t, a, v] 边一致
	
二 成像模型
    3.  描述针孔相机模型
		a. Z/f = X/X' = Y/Y' --> X' = fX/Z  Y' = fY/Z
		b. u = aX' + cx; v = bY' +cy;
		c. u = fxX/Z + cx; v = fyY/Z + cy;
		d. (u,v,1) = 1/Z(fx, 0, cx, 0, fy, cy, 0, 0, 1)(Xc, Yc, Zc) = 1/Z*K*Pc 
		e. Z*Puv = K(R*Pw +t) = K*T*Pw
	
	4.  什么是相机畸变？
		透镜形状-经向畸变（桶形， 枕形）； xc = x(1+k1r2 + k2r4 + k3r6) yc = y(1+k1r2 + k2r4 + k3r6)
		透镜与成像面不平行-切向畸变	: xc = x + 2p1xy + p2(r2+2x2) yc = y + 2p2xy + p1(r2+2x2)
	
	5. 介绍张正友标定法
		已知： 内参数矩阵：五参数模型
				畸变模型：四阶径向畸变模型
				标定物：平面靶标
				世界坐标系W按照下图置于靶标平面，原点设在靶标一角，Xw和Yw方向沿着靶标平面，Zw方向垂直于靶标平面（z=0平面上）
		求解： 相机内参与畸变参数
		步骤：  1. 不考虑畸变，标定摄像机参数，得到参数的线性初值
				2. 利用线性初值，进行非线性标定。得到畸变参数
				3. 重复1和2，直至参数收敛
		具体步骤：
			求解单应矩阵 ==由多组对应[u,v,1]齐次坐标求H， H8自由度， 由4对可得。 一般多对求最优解
			求解摄像机内参数 ==内参不变，多组照片求解多组H 求解内参
			求解摄像机外参数 == 已知内参和H，求解外参
			求解畸变系数k1, k2 == 将畸变参数代入，多组照片迭代优化求 K，R，t，k1，k2
			参数最优化
		单应性(homography)：在计算机视觉中被定义为一个平面到另一个平面的投影映射 同时包含了相机内参和外参
	
	6. 单目，双目，深度相机对比
		单目相机：
		常用型号：有非常多的种类可以选择
		优点：
		1、应用最广，成本可以做到非常低。
		2、体积小，标定简单，硬件搭建也简单。
		3、可以用于室内和室外（有适当光照条件下）。
		缺点：
		1、具有纯视觉传感器的通病：在光照变化较大，纹理特征缺失、快速运动导致模糊的情况下无法使用（睁眼瞎）。
		2、SLAM过程使用单目相机有尺度不确定性，需要专门初始化。
		3、必须通过运动才能估计深度（帧间匹配三角化）
		
		双目相机：
		常用型号：Indemind，小觅，ZED等
		优点：
		1、相比于单目，在静止时就能够根据左右相机视差图计算深度。
		2、可测量距离可以根据基线调节。基线距离越大，测量距离越远。
		3、可以用于室内和室外（有适当光照条件下）。
		缺点：
		1、双目相机标定相对复杂？
		2、用视差计算深度比较消耗资源
		3、具有纯视觉传感器的通病：在光照变化较大，纹理特征缺失、快速运动导致模糊的情况下无法使用（睁眼瞎）。
		
		RGB-D相机：
		常用型号：Kinect系列、Realsense系列、Orbbec、Pico等
		优点：
		1、使用物理测距方法测量深度，所以避免了纯视觉传感器的通病，在没有光照的情况下、快速运动的情况下都可以测距。这是非常大的优势。
		2、相对双目，输出帧率较高，更适合运动场景。
		3、输出深度值比较准，结合RGB信息，容易实现手势识别、人体姿态估计等应用。
		缺点：
		1、测量范围窄，易受日光干扰，通常只能用于室内场景
		2、在遇到透射材料、反光表面、黑色物体情况下表现不好，造成深度图缺失
		3、通常分辨率无法做到很高，目前主流分辨率VGA（640x480）
		4、标定比较复杂？
		
	7. 单目相机尺度性和如何初始化（拓展：双目，RGBD，VIO的初始化及传感器标定）
		尺度不确定：
		相机模型 u = fxX/Z + cx; v = fyY/Z + cy; 坐标为归一化坐标，与Z成比例。尺度不确定
		在对极几何中，求出的平移t是归一化的，没有单位，放大缩小系数也成立。
		初始化：（需要平移，用归一化t计算mappoint空间位置）
		1. 特征提取，匹配 
		2. 求解H和F 
		3. 打分机制选择模型
		4. 恢复出运动和地图的三维结构（sfm） 求解R和T，根据R，T三角化匹配好的关键点，得到这些关键点的3D坐标
	
	8. 双目如何求深度
		z = fb/d d= ul-ur; d为视差 b为基线
		
	9. 解释相机内外参数
		相机内参包括焦距fx，fy，cx，cy，径向畸变系数k1,k2,k3，切向畸变系数p1,p2
		其中内参一般来说是不会改变，但是当使用可变焦距镜头时每次改变焦距需要重新标定内参
		当图像裁剪时内参cx，cy会发生改变，比如图像从8*8变成4*4时，cx，cy需要除以2
		一般标定工业相机时只需要得到畸变系数k1，k2即可，对于畸变系数较大的鱼眼相机需要得到k3，p1，p2
		相机外参分为旋转矩阵R和平移矩阵t，旋转矩阵和平移矩阵共同描述了如何把点从世界坐标系
		转换到摄像机坐标系
	

	10. 我们知道相机的内参有 fx, fy, cx, cy, 畸变参数(只考虑k1, k2)，相对世界坐标原点外参T。如果我们现在对相机拍摄的图片进行2倍的下采样，那么这些参数会如何变化？
		fx,fy 焦距有关，不变， cy ,cy 一半， K1， K2变。 T变
	
	11. 我们知道双目相机两个相机光心的间距我们 称之为 baseline。如果双目相机baseline比较大，我们称之为wide baseline.现在某代码中使用一个单目相机进行SLAM过程，在特征匹配时资料中提到了wide baseline，请问这个wide baseline怎么理解？
		VIO相机模拟双目？ 通过 两次相机移动，imu估计baseline？
	
	12. RGB-D相机我们知道可以直接输出 RGB + depth两张图比如我们常见的Kinect 是结构光原理，包括一个彩色相机，一个红外发射器，一个红外接收器。另外，Intel的Realsense系列RGB-D相机也非常常用，比如下面Realsense D415，官网说是Active IR stereo，也就是双目深度相机，这个双目和我们平时说的双目有何不同？为什么有如下四个孔？
		D415的硬件包含了两个深度相机，一个RGB相机和一个结构光红外投影仪。深度卷帘相机（逐行扫描），红外结构光深度测距。D435 左右红外相机，红外点阵投影器，RGB相机
	
	13. 我们在阅读文献或者代码中误差相关时，经常可以看到一个概念，叫逆深度（inverse depth）。也就是深度的倒数，那么同学们有没有想过，为什么使用逆深度误差而不是深度误差？
		在极线搜索和块匹配中，我们假设深度值满足高斯分布，实际上，假设深度的倒数（也就是逆深度），为高斯分布是比较有效的。随后，在实际应用中，逆深度也具有更好的数值稳定性
	
	14. 项目中相机是如何选型的？测试了哪些相机？为什么选这个？
	    为啥选深度相机：单目尺度性。选深度相机， 即可slam也可以做检测，追踪，避障
		测试了哪些？
			Kinect系列 V1， V2； 深度最大512*424 距离0.5-4m
			Realsense F200， R200， R300， D415， D435 .  深度可到 1280*720 0.1-6m
			Orbbec Astra， Persee 深度可到 1280*720 一般640*480 30FPS  0.4-6m 
			ZED stereo camera RGB双目 最大20m 分辨率高 计算耗资源
		最后选步科， 百度定制，分辨率高，双目VIO
			
		
	
三 李群李代数
	15. 为什么要引入李群李代数
		旋转矩阵自身是带有约束的，正交且行列式为1，他们作为优化变量时，会引入额外的约束，时优化变的困难，通过李群李代数的转换关系，把位姿估计变成无约束的优化问题。
	16. SLAM中为什么要引入李群李代数？
		SLAM的过程就是不断的估计相机的位姿和建立地图。其中，相机位姿也就是我们所说的变换矩阵T
		拿着相机一边移动一边拍，假设某个时刻相机的位姿是T，它观察到一个在世界坐标系中的一个空间点p，并在相机上产生了一个观测数据z，
		那么z = Tp + noise，noise是观测噪声。那么观测误差就是e = z - Tp
		假设我们总共有N个这样的三维点p和观测值z，那么我们的目标就是寻找一个最佳的位姿T，使得整体误差最小化
		求解此问题，就是求目标函数J对于变换矩阵T的导数
		变换矩阵T，我们知道T所在的SE(3)空间，对加法计算并不封闭，也就是说任意两个变换矩阵相加后并不是一个变换矩阵，这主要是因为旋转矩阵对加法是不封闭造成的，它是有约束的
		李代数就是解决这个问题的。我们把大写SE(3)空间的T映射为一种叫做李代数的东西，映射后的李代数我们叫做小se(3)好了。
		它是由向量组成的，我们知道向量是对加法封闭的。这样我们就可以通过对李代数求导来间接的对变换矩阵求导了。
		
四 非线性优化
	17. 简述一下梯度下降，GN、LM等优化方法的区别
		（1） GN：线搜索
		将f（x）进行一节泰勒展开，最后求解
		线性方程H△x=b；
		用JT*J近似H矩阵，省略H复杂的计算
		过程；
		稳定性差，可能不收敛；
		（2） LM：信赖区域；
		求解线性方程(H+λI)△x=b；
		提供更稳定，更准确的增量
		G-N中的H矩阵可能为奇异矩阵或者病态矩阵，导致算法不收敛。而且当步长较大时，也无法保证收敛性，所以采用L-M求解增量方程，但是它的收敛速度可能较慢
	
	18. 简述一下Bundle Adjustment的过程
		BA的本质是一个优化模型，其目的是最小化重投影/光度误差，用于优化相机位姿和世界点。局部BA用于优化局部的相机位姿，提高跟踪的精确度；全局BA用于全局过程中的相机位姿，使相机经过长时间、长距离的移动之后，相机位姿还比较准确。BA是一个图优化模型，一般选择LM(Levenberg-Marquardt)算法并在此基础上利用BA模型的稀疏性进行计算；可以直接计算，也可以使用g2o或者Ceres等优化库进行计算。
		Bundle Adjustment : 从视觉重建中提炼出最优的3D模型和相机参数（内参和外参），好似每一个特征点都会反射几束光线，当把相机位姿和特征点位置做出最优的调整后，这些光线都收束到相机相机光心。也就是根据相机的投影模型构造构造代价函数，利用非线性优化（比如高斯牛顿或列文伯格马夸而尔特）来求最优解，利用雅克比矩阵的稀疏性解增量方程，得到相机位姿和特征点3D位置的最优解。
		BA可以分为基于滤波器的BA和基于迭代的BA
		
	19. EKF和BA的区别
		（1） EKF假设了马尔科夫性，认为k时刻的状态只与k-1时刻有关。非线性优化使用所有的历史数据，做全体的SLAM
		（2） EKF做了线性化处理，在工作点处用一阶泰勒展开式近似整个函数，但在工作点较远处不一定成立。非线性优化每迭代一次，状态估计发生改变，我们会重新对新的估计点做 泰勒展开
		可以把EKF看做只有一次迭代的BA		
		
	20. 优化求解过程中，g2o或者ceres的内部实现过程，有哪些加速计算的处理
		****稀疏化处理，边缘化
	
	21. 做图优化时，对比采用四元数法和李代数法在数学直观性、计算量上的差异性
		***
		
	22. 如何优化重投影误差？采用什么方法求解？如果误匹配的点重投影之后误差很大，如何解决它对整个优化问题的影响？
		图优化模型，将路标点和相机位姿作为两个节点，观测模型作为边，同时优化两个变量
		SLAM中常用L-M求解，如果误匹配误差很大可以考虑用核函数（Huber）
		
	23. 重投影误差的表达式，误差关于位姿的偏导数怎么算？误差关于空间点的偏导数怎么计算？
		重投影误差 ： 
			像素坐标（观测到的投影位置）与3D点按照当前估计的位姿进行投影得到的位置相比较得到的误差。
		最小化重投影误差问题（Bundle Adjustment问题）：
			可以将位姿和三维特征点P同时优化可以将位姿和三维特征点P同时优化
		误差模型：
			siui = K* exp(s^)Pi 
			s = argmin sum[1/2(ui- 1/si * K* exp(s^)Pi )**2]
		偏导数：
			泰勒展开
			雅可比矩阵
			李群李代数
	
	24. 一阶、二阶优化，Jacobian、hessian矩阵
		求解增量将目标函数在x附近进行泰勒展开
		f(x+dx)**2 = f(x)**2 + J(x)dx + 1/2 dxt H dx
		J是f(x)**2 关于x的导数（Jacobian） H是二阶导数 hessian矩阵 。
		一阶优化是展开一阶项 dx = - JT(x)
		二阶优化是展开二阶项 H*dx = -JT(x)
	
	25. 为什么SLAM中常用LM算法而不是高斯牛顿求解优化问题？
		G-N中的H矩阵可能为奇异矩阵或者病态矩阵，导致算法不收敛。
		而且当步长较大时，也无法保证收敛性，所以采用L-M求解增量方程，但是它的收敛速度可能较慢。
	
	26. 用g2o和ceres库都能用来进行BA优化，这两者在使用过程中有什么不同？
		***
		
五 特征匹配 
	27. 描述一下SIFT或者SURF特征检测，差异
	    特征点引出：
			SIFT/SURF作者的想法是首先找到图像中的一些“稳定点”，
			这些点是一些特殊的点，不会因为视角的改变、光照的变化、噪音的干扰而消失，
			比如角点、边缘点、暗区域的亮点以及亮区域的暗点。
			SIFT/SURF提取的稳定点，首先都要求是局部极值。
			但是，当两个物体的大小比例不一样时，大图像的局部极值点在小图像的对应位置上有可能不是极值点。
			于是SIFT/SURF都采用图像金字塔的方法，每一个截面与原图像相似，这样两个金字塔中就有可能包含大小最近似的两个截面了。
			SIFT/SURF的作者都想到以特征点为中心，在周围邻域内统计特征，将特征附加到稳定点上，生成特征描述子。
			在遇到旋转的情况下，作者们都决定找出一个主方向，然后以这个方向为参考坐标进行后面的特征统计，就解决了旋转的问题。
		SIFT特征检测算法的特点:
			SIFT特征是图像的局部特征，对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性
			信息量丰富，适用于在海量特征数据库中进行匹配
			多量性，少数物体也可以产生大量SIFT特征
			高速性，经优化的SIFT匹配算法甚至可以达到实时性
		SIFT特征检测的步骤:
			检测尺度空间的极值点
			精确定位特征点(Keypoint)
			设定特征点的方向参数
			生成特征点的描述子（128维向量）
		SURF：
			SURF特征(Speeded Up Robust Features，加速鲁棒性特征)是对SIFT特征的进一步优化，
			基于Hessian矩阵构造金字塔尺度空间，利用箱式滤波器（box filter）简化二维高斯滤波，不需要再进行降采样；
			通过Haar小波特征设定特征点主方向，这样构建的特征描述子就是64维的；
			surf构造的金字塔图像与sift有很大不同，就是因为这些不同才加快了其检测的速度。
			Sift采用的是DOG图像，而surf采用的是Hessian矩阵行列式近似值图像，也写作DOH算子。
		差异：
			尺度空间--》 SIFT： DOG与不同尺度的图片卷积  SURF：不同尺度的box filters与原图片卷积
			特征点检测--》SIFT： 先进行非极大抑制，再去除低对比度的点。再通过Hessian矩阵去除边缘的点 
						  SURF： 先利用Hessian矩阵确定候选点，然后进行非极大抑制
			描述子； SIFT：128维  SURF： 64维

	28. 描述RANSAC（outlier+鲁棒核）
		RANSAC算法的基本假设是样本中包含正确数据(inliers，可以被模型描述的数据)，也包含异常数据(outliers，偏离正常范围很远、无法适应数学模型的数据)，即数据集中含有噪声。这些异常数据可能是由于错误的测量、错误的假设、错误的计算等产生的。同时RANSAC也假设，给定一组正确的数据，存在可以计算出符合这些数据的模型参数的方法。
		优缺点：
		RANSAC算法的优点是能鲁棒的估计模型参数。例如，他能从包含大量局外点的数据集中估计出高精度的参数。缺点是它计算参数的迭代次数没有上限，如果设置迭代次数的上限，得到的结果可能不是最优的结果，甚至可能得到错误的结果。RANSAC只有一定的概率得到的可信的模型，概率与迭代次数成正比。另一个缺点是它要求设置跟问题相关的阈值，RANSAC只能从特定的数据集中估计出一个模型，如果存在两个（或多个）模型，RANSAC不能找到别的模型。
	
	29. 介绍PNP算法
		Perspective-n-Points, PnP(P3P)提供了一种解决方案，它是一种由3D-2D的位姿求解方式，即需要已知匹配的3D点和图像2D点。目前遇到的场景主要有两个，其一是求解相机相对于某2维图像/3维物体的位姿；其二就是SLAM算法中估计相机位姿时通常需要PnP给出相机初始位姿。
		在场景1中，我们通常输入的是物体在世界坐标系下的3D点以及这些3D点在图像上投影的2D点，因此求得的是相机坐标系相对于世界坐标系(Twc)的位姿
		在场景2中，通常输入的是上一帧中的3D点（在上一帧的相机坐标系下表示的点）和这些3D点在当前帧中的投影得到的2D点，所以它求得的是当前帧相对于上一帧的位姿变换，如图所示：
		两种情况本质上是相同的，都是基于已知3D点和对应的图像2D点求解相机运动的过程。
	
	30. 如何对匹配好的点做进一步的处理，更好保证匹配效果
		（1）确定匹配最大距离，汉明距离小于最小距离的两倍
		（2）使用KNN-matching算法，令K=2。则每个match得到两个最接近的descriptor，然后计算最接近距离和次接近距离之间的比值，当比值大于既定值时，才作为最终match。
		（3）RANSAC（使用RANSAC找到最佳单应性矩阵。由于这个函数使用的特征点同时包含正确和错误匹配点，因此计算的单应性矩阵依赖于二次投影的准确性）
	31. 介绍单应矩阵，本质矩阵，基本矩阵。只旋转不平移能不能求F，只旋转不平移能不能求H
		本质矩阵 E=t^R   基本矩阵F = K-T E K-1
		单应矩阵 H = R-t*nT/d 描述特征点在同一平面上
		在相机只有旋转而没有平移的情况，此时t为0，E也将为0，导致无法求解R，这时可以使用单应矩阵H求旋转，但仅有旋转，无法三角化求深度。
		一般同时估计E和H，选择重投影误差比较小的R，t
		
	32. 什么是极线约束
		极线约束也叫对极约束。这个约束的意思就是说，假设相机在不同位置拍摄了两幅图像，如果一个空间点P在两幅图上分别有两个成像点，已知左图成像点为p1，那么右图成像点p2一定在相对于p1的极线上。
		极线约束的好处：从上面的描述我们可以看到，我们在做特征点匹配时，左图成像点p1的待匹配点p2一定在相对于p1的极线上，那么我们在做搜索时就可以在极线附近（考虑实际可能 会有一点误差）进行搜索，相对暴力匹配极大减少待匹配的点的数量。
		极线约束可以简洁的给出匹配点的空间位置关系，使得相机位姿估计问题变的简单。
	
	33. 特征匹配（稀疏）和稠密匹配区别
		特征匹配：
		（1）速度快，效率高，可以到亚像素级别，精度高
		（2）匹配元素为物体的几何特征，对照明变化不敏感
		稠密匹配
		（1）速度慢，效率低
		（2）对无纹理区域匹配效果不理想，对光强条件敏感	
		
	34. ICP算法（二维码、手眼标定）
		ICP算法：
			迭代最近点，是点云匹配算法。RGBD相机得到一组空间点云P，运动后得到第二组空间点云Q。
			点云先匹配一一对应，优化min  1/2 sum(qi - Rpi-t)**2 求R，t
			求解R t： SVD分解 非线性优化
			SVD分解；1. 计算两组点质心位置p，p' , 计算每点去质心坐标   2. 优化求解R 3. 求解t
			非线性优化： 求解误差项极小值。 初始点随便选，存在唯一解或无解。

		手眼标定： 
			机器人与相机坐标系进行标定
			通常机器人的手眼关系分为eye-in-hand以及eye-to-hand两种
			对于eye-in-hand情况，机器人手眼标定即标定得到机器人末端与相机之间的坐标变换关系；
			对于eye-to-hand情况，机器人手眼标定即标定得到机器人基座与相机之间的坐标变换关系
			eye-to-hand可以使用九点标定实现
	
	35. 什么是ICP 算法？简述一下算法原理，SLAM中一般什么情况下会使用该算法？
		在点云中，需要迭代匹配点云。 在vslam 中 特征点匹配已知，不需要迭代，直接求Rt
		ICP算法：
				迭代最近点，是点云匹配算法。RGBD相机得到一组空间点云P，运动后得到第二组空间点云Q。
				点云先匹配一一对应，优化min  1/2 sum(qi - Rpi-t)**2 求R，t
				求解R t： SVD分解 非线性优化
				SVD分解；1. 计算两组点质心位置p，p' , 计算每点去质心坐标   2. 优化求解R 3. 求解t
				非线性优化： 求解误差项极小值。 初始点随便选，存在唯一解或无解。
	
	36. RANSAC的iteration怎么算
		iteration计算：
			w = inliers / data
			靠经验, 当随机选点n不变，k越大 p（算法成功概率）越大；
			w不变，n越大，所需k越大。 w未知，n选小一点好。k选择大一点
	
	37. P3P问题有几个解
		最多4个解， 通过三角化验证点求最佳
	
六 直接法
	38. 描述特征点法和直接法的优缺点
		特征点法
		优点：
		（1）精确，直接法属于强假设
		（2）运动过大时，只要匹配点在像素内，则不太会引起误匹配，鲁棒性好
		缺点：
		（1）关键点提取、描述子、匹配耗时长
		（2）特征点丢失场景无法使用
		（3）只能构建稀疏地图
		直接法
		优点：
		（1）省去计算特征点、描述子时间
		（2）可以用在特征缺失的场合（比如白墙）
		（3）可以构建半稠密乃至稠密地图
		缺点：
		（1）易受光照和模糊影响
		（2）运动必须微小，要求相机运动较慢或采样频率较高（可以用图像金字塔改善）
		（3）非凸性；单个像素没有区分度
		
	39. 特征点法和直接法的BA有何不同
		（1） 误差函数不同。特征点法是重投影误差，直接法是光度误差
		（2） 雅克比矩阵不同
		
	40. 光流和直接法有何不同
		光流仅估计了像素间的平移，但
		（1）没有用相机结构
		（2）没有考虑相机的旋转和图像缩放
		（3）边界点追踪效果差
		光流提取特征点， 直接法用像素梯度
		
	41. 光流的假设、仿射变换、4种方法，svo采取的方法，优势何在
		LK光流灰度不变假设： 同一个空间点的像素灰度值 在各图像中是固定不变的。
		优势在于： 节约描述子计算与匹配，计算快。能直接得特征点的对应关系，误匹配少，鲁棒性差，要求运动慢。
	
	42. 直接法估计相机位姿时，并不需要 提取特征点，而是通过优化匹配点的像素值误差（也称光度误差）估计位姿，但也会面临快速运动，光照变化等的挑战，如果让你改善该问题，你会采用哪些方法来提高跟踪质量(精度，速度，鲁棒性等)？
		低端计算平台用稀疏直接法，求解相机位姿；计算性能高用稠密法建立完整地图 
		优点： 1. 省去计算特征点，描述子的时间 2. 只用像素梯度，适合特征缺失场合 3. 可建半稠密和稠密地图
		缺点： 1. 非凸性，陷入局部极小值。适合运动小。 2. 灰度不变假设光照和曝光要求高
		改善： 1. 小运动 2. 尺度金字塔改善非凸 3. 固定曝光，光照，光度标定
		
七 回环检测
	43. 闭环检测常用方法（orb、lsd、深度学习）
		回环检测方法： 词袋模型 -- 深度学习
	
	44. Kmeans伪代码
		K-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。
		让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。
		关键假设： 数据之间的相似度可以使用欧氏距离度量。 
		算法； 随机K质心 ，按距离划分n点给K个质心 ， 每个分类计算新质心，重复计算知道新老质心距离小
		伪代码：
			function K-Means(输入数据，中心点个数K)
				获取输入数据的维度Dim和个数N
				随机生成K个Dim维的点
				while(算法未收敛)
					对N个点：计算每个点属于哪一类。
					对于K个中心点：
						1，找出所有属于自己这一类的所有数据点
						2，把自己的坐标修改为这些数据点的中心点坐标
				end
				输出结果：
			end
		缺点： 1. 指定K， 不指定用AP聚类
				2. 初始化选择中心是随机，可能导致收敛慢。K-mean++ 解决这个问题。
		
	
	45. SLAM中回环检测（闭环检测）的目的是什么？简述一下SLAM中可以使用的回环检测方法？
		SLAM过程中，尤其单目存在尺度漂移，立体视觉也存在误差，且随着时间推移误差是累加
		因为前端给的是相邻帧间估计，后端优化估计最大后验概率，无法完全消除累计误差
		回环检测的目的是为了消除累计误差。方法是增加远距离帧的约束，在回环检测中检测图像相似度，叠加远端约束。
		加入远端约束后，一起优化求解最优估计。
		回环检测方法： 词袋模型 -- 深度学习
	
八 SLAM基础
	46. 描述卡尔曼滤波
		卡尔曼滤波是建立在线性的状态空间和高斯分布噪声上。
		卡尔曼滤波做状态估计，用观测（方程）来优化状态（运动方程）估计。
		运动方程 ： x(k) = A * x(k-1) + w  Pk = A * P(k-1) * At
		观测方程 ： Z(k)= Hx(k) + Q
		卡尔曼增益： 有误差求卡尔曼增益， 运动和观测加权平均。
		
	
	47. 描述一下粒子滤波
		四大步骤： 预测， 更新， 重要性采样， 重采样
		首先初始化粒子在地图的分布，可以采用先验按一定分布或者在空间中随机分布，得到其分布权重
		机器人运动后（状态变化后），对每个粒子，根据运动方程预测新位置，得到新粒子分布
		观测当前环境，对每个粒子，计算通过计算观测与预测位置误差，重新计算每个粒子权重
		按照权重对粒子进行重采样，得到新粒子分布与权重。粒子加权平均和协方差可获得状态估计。
	
	48. 说一说某个SLAM的工作原理（orb,lsd,svo,ptam）及其优缺点，如何改进
		工作原理：
			ORB_SLAM2由3+1个平行线程组成， 跟踪，局部建图，回环检测，全局BA优化。
			前三个并行，最后一个在回环检测后触发全局BA优化
			a 跟踪: 通过局部地图特征进行匹配，用纯运动BA最小化重投影误差进行定位每帧图片的相机；适配单目双目RGBD
			b 局部建图: 通过执行局部BA管理局部地图并优化(优化Rt和局部地图点)
			c 回环检测：检测大的环并通过执行位姿图优化更正漂移误差。这个线程触发第四线程；
			d 全局BA：在位姿图优化之后，计算整个系统最优结构和运动结果
		优点： 
			1. 使用ORB特征，计算快速，旋转不变性和光照适应性
			2. 加入DBOW回环检测消除累积误差，重定位快速效果好。
			3. 优秀代码和完整slam架构，适合改造，移植。
			4. 关键帧的选取，g20后端优化
		缺点：
			1. 稀疏点云图不能导航
			2. 初始化时最好保持低速运动，对准特征和几何纹理丰富的物体
			3. 旋转时比较容易丢帧，特别是对于纯旋转，对噪声敏感，不具备尺度不变性
			4. 如果使用纯视觉 slam 用于机器人导航，可能会精度不高，或者产生累积误差
		改进：
			1. 引入IMU，里程计数据融合估计R，t 解决白墙等纹理不丰富失效，旋转丢帧等问题
			2. 引入融合激光地图解决地图稀疏性无法导航问题
	
	49. 什么是连通区域算法
		连通区域一般是指图像中具有相同像素值且位置相邻的前景像素点组成的图像区域（Region，Blob）。
		连通区域分析是指将图像中的各个连通区域找出并标记。
		常用于：OCR识别中字符分割提取（车牌识别、文本识别、字幕识别等）、视觉跟踪中的运动前景目标分割与提取（行人入侵检测、遗留物体检测、基于视觉的车辆检测与跟踪等）、医学图像处理（感兴趣目标区域提取）、等等。
		也就是说，在需要将前景目标提取出来以便后续进行处理的应用场景中都能够用到连通区域分析方法，通常连通区域分析处理的对象是一张二值化后的图像。
		
	
	50. SVO， LSD中的深度滤波器原理
		1. 深度地图更新
			三角化计算深度： 通过参考帧极线 --》极线上的最佳匹配--》 计算深度 
			深度误差： 相机位姿、内参数等带来的几何误差， 图像带来的光度误差，融合为深度误差
			深度观测融合： 逆深度服从高斯分布，多次观测逆深度值用卡尔曼滤波融合
		2. 深度融合
			深度由一帧传递到下一帧时，两个点经过深度传播时传递到同一个像素点，则分两种情况处理：两者相近（在 [公式] 内）进行不确定度的加权融合；否则舍弃那个较远的
		3. 规则化
			对每个深度值取周围深度值的加权平均；相差较大的除外，以保留sharp edges。
		
	
	51. 如何处理关键帧
		关键帧目前是一种非常常用的方法，可以减少待优化的帧数，并且可以代表其附近的帧。可以理解为一个学校里有100个班级，每个班的班长就是一个关键帧，他可以代表他班里的人，那么如何选取关键帧呢？
		选取的指标主要有：
		（1）距离上一关键帧的帧数是否足够多（时间）。比如我每隔固定帧数选择一个关键帧，这样编程简单但效果不好。比如运动很慢的时候，就会选择大量相似的关键帧，冗余，运动快的时候又丢失了很多重要的帧。
		（2）距离最近关键帧的距离是否足够远（空间）/运动
		比如相邻帧我根据pose计算运动的相对大小，可以是位移也可以是旋转或者两个都考虑，运动足够大（超过一定阈值）就新建一个关键帧，这种方法比第一种好。但问题是如果对着同一个物体来回扫就会出现大量相似关键帧。
		（3）跟踪质量（主要根据跟踪过程中搜索到的点数和搜索的点数比例）/共视特征点
		这种方法就是记录当前视角下的特征点数，或者视角，当相机离开当前场景时才会新建关键帧，避免了第2种方法的问题。缺点是比较复杂
		打个比方，关键帧相当于slam的骨架，是在局部一系列普通帧中选出一帧作为局部帧的代表，记录局部信息。举例来说，摄像头放在原处不动，普通帧还是要记录的，但关键帧因为总看到原场景，所以不会增加。
		三角化需要一定程度的共视区域，所以普通帧每2帧之间会存在大量的信息冗余，如果所有帧全部参与计算，不仅浪费了算力，对内存也是极大的考验，这一点在前端vo递归处理方式中表现不明显，但在后端优化里是一个大问题，所以关键帧主要作用是面向后端优化的算力与精度的折中。此外，关键帧选择时还会对图片质量、特征点质量等进行考察，一定程度上也发挥了滤波的作用，防止无用的或错误的信息进入优化过程而破坏定位建图的准确性。
		选择关键帧主要从关键帧自身和关键帧与其他关键帧的关系2方面来考虑。一方面，关键帧自身质量要好，例如不能是非常模糊的图像、特征点数量要充足、特征点分布要尽量均匀等等；另一方面，关键帧与其他关键帧之间的关系，需要和局部地图中的其他关键帧有少量的共视关系，但大部分特征点是新特征点，以达到既存在约束，又尽量少的信息冗余的效果，例如局部地图点投影到此帧的点数低于一个阈值或前一个关键帧的特征点在此帧里已经有90%观测不到等等。
		在关键帧的运用上，我认为orbslam做的非常好，尤其是在回环检测中使用了以关键帧为代表的帧“簇”的概念，回环筛选中有一步将关键帧前后10帧为一组，计算组内总分，以最高分的组的0.75为阈值，滤除一些组，再在剩下的组内各自找最高分的一帧作为备选帧，这个方法非常好地诠释了“关键帧代表局部”的这个理念。
	
	52. SLAM中的绑架问题
		绑架问题就是重定位，是指机器人在缺少之前位置信息的情况下，如何去确定当前位姿。例如当机器人被安置在一个已经构建好地图的环境中，但是并不知道它在地图中的相对位置，或者在移动过程中，由于传感器的暂时性功能故障或相机的快速移动，都导致机器人先前的位置信息的丢失，在这种情况下如何重新确定自己的位置。
	
	53. 常用的边缘检测算子和优缺点
		边缘检测一般分为三步，分别是滤波、增强、检测。基本原理都是用高斯滤波器进行去噪，之后在用卷积内核寻找像素梯度。常用有三种算法：canny算子，sobel算子，laplacian算子
		canny算子：一种完善的边缘检测算法，抗噪能力强，用高斯滤波平滑图像，用一阶偏导的有限差分计算梯度的幅值和方向，对梯度幅值进行非极大值抑制，采用双阈值检测和连接边缘。
		sobel算子：一阶导数算子，引入局部平均运算，对噪声具有平滑作用，抗噪声能力强，计算量较大，但定位精度不高，得到的边缘比较粗，适用于精度要求不高的场合。
		laplacian算子：二阶微分算子，具有旋转不变性，容易受噪声影响，不能检测边缘的方向，一般不直接用于检测边缘，而是判断明暗变化。
	
	54. AR系统如何实现？
		1. 制作方要对扫描的实物进行建模。
			我们需要先扫描该物品的3D模型，并对其进行关键帧标定，
			比如:瓶盖、瓶身某位置或者图片的不同画面,
			然后根据不同的识别准备对应的动态视频数据
		2. 当扫描标定的关键帧,系统就会去寻找AR场景中与之最接近的关键帧，
			根据关键帧上的特征点，然后利用特定的算法找到AR场景中对应的视频，
			并将其展现在用户的手机画面中;
		3. 在这个AR应用中技术核心就是<识别跟踪>技术，
			AR应用首先要识别标示物，然后进行跟踪（跟踪用户扫描到的产品标记)，
			接着就在用户手机上展示对应的视频场景。
			AR应用是通过“特征点”进行识别的;
			检测到特征点还不行，如果要判断两张图片是否是同一张图片，
			识别设备还要判断两张图片的特征点是否一致;
		4. 在完成一幅图片的比对后还要对视频帧的其他图片进行跟踪比对。
			特征跟踪有两种方式:
			一种是对视频流中的每一帧图像进行特征点匹配;
			二种则在第一幅图像中，寻找可能的特征位置
			然后在后续的图像中搜索它们的对应位置。
			这样完成对识别图片的跟踪后，
			AR应用就会在用户的手机上显现对应的视频画面了。
		5. AR应用会先将图片进行特征点的识别，并存储在应用中。
			这样当用户使用手机扫描到符合特征点的瓶身图片，
			AR应用会将当前图片和存储的图片进行比对，如果是一致的图片，
			AR应用就会将特定的视频展示在用户手机屏幕上。
			当然AR应用还会进行跟踪，如扫描不同的区域显示不同的视频，
			或者在扫描其他图片时快速识别并显示预置的视频到用户手机上。
	
	55. 介绍下VO
	
	56. MSCKF与ROVIO
		ROVIO：
			是基于EKF的视觉IMU紧耦合框架，通过IMU预测状态向量，通过视觉光度误差约束更新。
			创新在路标点参数化：相机坐标系下用bearing vector 和 逆深度表示，并在IMU预测阶段对路标点预测，视觉更新时修正
			计算量小(EKF，稀疏的图像块)，但是对应不同的设备需要调参数，参数对精度很重要。没有闭环，没有mapping thread。经常存在误差会残留到下一时刻。
		
	57. MSCKF与预积分
		预积分：
			通过欧拉积分，有j时刻位姿通过IMU数据估计i时刻位姿。
			IMU就是提供了两个关键帧的相对测量，从而构建误差函数对关键帧姿态的迭代优化。
		传统EKF： 
			多个特征点同时约束一个相机位姿，进行KF更新
			系统状态向量(state vector) ： 位姿pose、速度velocity、以及 3D map points 坐标等
			融合IMU是用系统状态向量IMU 做 预测predict step
			再用 image frame 中观测 3D map points 的观测误差做 更新update step。
			
		MSCKF： 
			一个特征点同时约束多个相机位姿(多相机观测同时优化，窗口多帧优化)，进行KF更新
			更新阶段update step 推迟到某一个 3D map point 在多个 frame 中观测之后进行计算
			在 update 之前每接收到一个 frame，只是将 state vector 扩充并加入当前 frame 的 pose estimate。
			 在update step时，相当于基于多次观测同时优化 pose 和 3D map point
		MSCKF算法框架：
			1. 初始化
				1.1 摄像机参数、
					噪声方差（图像噪声、IMU噪声、IMU的bias）、
					初始的IMU协方差、
					IMU和摄像机的外参数*、I
					IMU和摄像机的时间偏移量*
				1.2 MSCKF参数：
					状态向量里滑动窗口大小的范围、
					空间点三角化误差阈值、
					是否做零空间矩阵构造 和 QR分解
				1.3 构造MSCKF状态向量
			2.读取IMU数据，估计新的MSCKF状态变量和对应的协方差矩阵
			3.图像数据处理
				3.1 MSCKF状态向量 中 增加当前帧的摄像机位姿；
					若位姿数大于滑动窗口大小的范围，
					去除状态变量中最早的视图对应的摄像机位姿.
				3.2提取图像特征并匹配，去除外点.
				3.3 处理所有提取的特征。
					  判断当前特征是否是之前视图中已经观察到的特征
					  3.3.1 如果当前帧还可以观测到该特征，则加入该特征的track列表
					  3.3.2 如果当前帧观测不到该特征(Out_of_View)，
							将该特征的track加入到featureTracksToResidualize，用于更新MSCKF的状态变量.
					  3.3.3 给该特征分配新的featureID，并加入到当前视图可观测特征的集合
				3.4 循环遍历featureTracksToResidualize中的track，用于更新MSCKF的状态变量
					  3.4.1 计算每个track对应的三维空间点坐标
						   (利用第一幅视图和最后一幅视图计算两视图三角化，使用逆深度参数化和高斯牛顿优化求解)，
						   若三角化误差小于设置的阈值，则加入map集合.
					  3.4.2 计算视觉观测(即图像特征)的估计残差，并计算图像特征的雅克比矩阵.
					  3.4.3 计算图像特征雅克比矩阵的左零空间矩阵和QR分解，构造新的雅克比矩阵.
				3.5 计算新的MSCKF状态向量的协方差矩阵.
					  3.5.1 计算Kalman增益.
					  3.5.2 状态矫正.
					  3.5.3 计算新的协方差矩阵.
				3.6 状态变量管理
					  3.6.1 查找所有无feature track可见的视图集合deleteIdx.
					  3.6.2 将deleteIdx中的视图对应的MSCKF中的状态去除掉.
					  3.6.3 绘制运动轨迹.
